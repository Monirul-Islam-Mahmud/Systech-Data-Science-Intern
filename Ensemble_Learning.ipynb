{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "252f409e",
      "metadata": {
        "id": "252f409e"
      },
      "source": [
        "## Ensemble Learning\n",
        "\n",
        "Ensemble refers to a collection of elements considered as a unified entity rather than individual components. An Ensemble method entails the creation of multiple models, which are then combined to address a problem. These ensemble techniques enhance the model's robustness and capacity for generalization.\n",
        "<br>\n",
        "<img src=\"https://github.com/EDGE-Programe/Python-Basics/blob/master/Python_edge_program/Data%20Science/notebook_images/nb_24/ensemble1.png?raw=1\" alt=\"KNN\" width=58% height=49% title=\"Ensemble Learning\">\n",
        "<br>\n",
        "So, ensemble learning is a powerful machine learning technique that leverages the combination of multiple models (learners) to improve overall predictive performance and enhance the robustness of a machine learning system. It's based on the idea that combining several weak learners (models that are slightly better than random guessing) can often yield a strong learner (a model with high predictive accuracy).\n",
        "\n",
        "Now, let's use a **multivariate dataset** called QCM alcohol dataset from UCL repository to study ensemble methods."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ea421f1",
      "metadata": {
        "id": "1ea421f1"
      },
      "source": [
        "### Basic Ensemble Methods\n",
        "\n",
        "**1. Averaging:** Averaging Technique: This approach is primarily applied in regression scenarios. It involves creating several models separately and then providing the average of their predictions as the result. Typically, this combined output outperforms individual outputs due to a reduction in variance. In the following illustration, we train three regression models (linear regression, xgboost, and random forest) and compute their predictions. The ultimate prediction result is referred to as **`pred_final`**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64c4f402",
      "metadata": {
        "id": "64c4f402"
      },
      "outputs": [],
      "source": [
        "# combining all csv files into one\n",
        "import pandas as pd\n",
        "\n",
        "qcm3 = pd.read_csv('../datasets/qcm_alcohol/QCM3.csv', sep = ';')\n",
        "qcm6 = pd.read_csv('../datasets/qcm_alcohol/QCM6.csv', sep = ';')\n",
        "qcm7 = pd.read_csv('../datasets/qcm_alcohol/QCM7.csv', sep = ';')\n",
        "qcm10 = pd.read_csv('../datasets/qcm_alcohol/QCM10.csv', sep = ';')\n",
        "qcm12 = pd.read_csv('../datasets/qcm_alcohol/QCM12.csv', sep = ';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a557bad",
      "metadata": {
        "scrolled": true,
        "id": "0a557bad",
        "outputId": "e322e251-84fe-4d44-d1ae-c70266f32ec4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of dataset:  (125, 15)\n"
          ]
        }
      ],
      "source": [
        "dataset = pd.concat([qcm3, qcm6, qcm7, qcm10, qcm12])\n",
        "print(\"Shape of dataset: \", dataset.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55fa6466",
      "metadata": {
        "id": "55fa6466"
      },
      "outputs": [],
      "source": [
        "# training data\n",
        "\n",
        "X = dataset.iloc[:, 0:10].values\n",
        "y = dataset.iloc[:, [10,11,12,13,14]].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2392a6c",
      "metadata": {
        "scrolled": true,
        "id": "c2392a6c",
        "outputId": "c5362485-7724-4855-db06-9990b601e9cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xgboost in /home/rubayet/miniconda3/envs/edge/lib/python3.8/site-packages (2.0.0)\r\n",
            "Requirement already satisfied: numpy in /home/rubayet/miniconda3/envs/edge/lib/python3.8/site-packages (from xgboost) (1.23.5)\r\n",
            "Requirement already satisfied: scipy in /home/rubayet/miniconda3/envs/edge/lib/python3.8/site-packages (from xgboost) (1.10.1)\r\n"
          ]
        }
      ],
      "source": [
        "! pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab33de30",
      "metadata": {
        "id": "ab33de30",
        "outputId": "7bc751b3-955c-4ce0-dcaf-d7c3361406f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE for Model_1: 0.09631921967994472\n",
            "MSE for Model_2: 0.07602632533272329\n",
            "MSE for Model_3: 0.035407999999999995\n",
            "MSE for Average Ensemble: 0.04763073766233296\n"
          ]
        }
      ],
      "source": [
        "# importing utility modules\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "# Splitting between train data into training and validation dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
        "\n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = LinearRegression()\n",
        "model_2 = xgb.XGBRegressor()\n",
        "model_3 = RandomForestRegressor()\n",
        "\n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        "\n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        "\n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        "\n",
        "# printing the mean squared error between real value and predicted value\n",
        "print(\"MSE for Model_1:\",mean_squared_error(y_test, pred_1))\n",
        "print(\"MSE for Model_2:\",mean_squared_error(y_test, pred_2))\n",
        "print(\"MSE for Model_3:\",mean_squared_error(y_test, pred_3))\n",
        "print(\"MSE for Average Ensemble:\",mean_squared_error(y_test, pred_final))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e351dfca",
      "metadata": {
        "id": "e351dfca"
      },
      "source": [
        "**`2.Majority Voting:`** In the voting process, multiple models are trained individually, and their predictions are amalgamated to produce a concluding prediction, typically through one of these methods:\n",
        "<br>\n",
        "<img src=\"https://github.com/EDGE-Programe/Python-Basics/blob/master/Python_edge_program/Data%20Science/notebook_images/nb_24/ensemble2.jpg?raw=1\" alt=\"KNN\" width=49% height=31% title=\"Ensemble Learning\">\n",
        "<br>\n",
        "\n",
        "- **Hard Voting:** In hard voting, the ultimate prediction is the most frequently occurring prediction among all the models. Let's say we have three classifiers that made predictions for the output class, and their predictions are (A, A, B). In this case, the majority of the classifiers have predicted class A as the output. Therefore, the final prediction will be class A.\n",
        "<br>\n",
        "<img src=\"https://github.com/EDGE-Programe/Python-Basics/blob/master/Python_edge_program/Data%20Science/notebook_images/nb_24/ensemble4.jpg?raw=1\" alt=\"Soft Voting\" width=31% height=31% title=\"Hard Voting Classifier\">\n",
        "<br>\n",
        "\n",
        "- **Soft Voting:** In contrast, soft voting involves each model producing a probability distribution instead of a binary prediction. The predicted class is then the one with the highest probability. Let's consider an example where we have input data for three models, and their prediction probabilities for class A are (0.30, 0.47, 0.53), while for class B, they are (0.20, 0.32, 0.40). If we calculate the averages for class A and class B, we obtain 0.4333 for class A and 0.3067 for class B. It's evident that class A is the winner because it has the highest average probability among all the classifiers.\n",
        "<br>\n",
        "<img src=\"https://github.com/EDGE-Programe/Python-Basics/blob/master/Python_edge_program/Data%20Science/notebook_images/nb_24/ensemble3.jpg?raw=1\" alt=\"Soft Voting\" width=31% height=31% title=\"Soft Voting Classifier\">\n",
        "<br>\n",
        "\n",
        "- **Weighted Voting:** Weighted voting operates under the premise that certain models possess higher predictive accuracy than others. Therefore, these more skillful models are assigned a greater influence or weight when contributing to the final prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "638dbe9d",
      "metadata": {
        "id": "638dbe9d",
        "outputId": "1de1fd19-b904-4d0e-8e58-97bae86eb4df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hard Voting Score  1.000000\n",
            "Soft Voting Score  1.000000\n"
          ]
        }
      ],
      "source": [
        "# importing libraries\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# loading iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "Y = iris.target\n",
        "\n",
        "# train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.20, random_state = 42)\n",
        "\n",
        "# group / ensemble of models\n",
        "estimator = []\n",
        "estimator.append(('LR',LogisticRegression(solver ='lbfgs',multi_class ='multinomial',max_iter = 200)))\n",
        "estimator.append(('SVC', SVC(gamma ='auto', probability = True)))\n",
        "estimator.append(('DTC', DecisionTreeClassifier()))\n",
        "\n",
        "# Voting Classifier with hard voting\n",
        "vot_hard = VotingClassifier(estimators = estimator, voting ='hard')\n",
        "vot_hard.fit(X_train, y_train)\n",
        "y_pred = vot_hard.predict(X_test)\n",
        "\n",
        "# using accuracy_score metric to predict accuracy\n",
        "score = accuracy_score(y_test, y_pred)\n",
        "print(\"Hard Voting Score % f\" % score)\n",
        "\n",
        "# Voting Classifier with soft voting\n",
        "vot_soft = VotingClassifier(estimators = estimator, voting ='soft')\n",
        "vot_soft.fit(X_train, y_train)\n",
        "y_pred = vot_soft.predict(X_test)\n",
        "\n",
        "# using accuracy_score\n",
        "score = accuracy_score(y_test, y_pred)\n",
        "print(\"Soft Voting Score % f\" % score)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03cef598",
      "metadata": {
        "id": "03cef598"
      },
      "source": [
        "### Weighted Voting Classifier\n",
        "\n",
        "Weighted voting is required when our ensemble models are drastically different interms of their performance. So, we need to implement a way to prioritize the model with the best accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c82930d4",
      "metadata": {
        "id": "c82930d4",
        "outputId": "af031fc5-eaea-4622-b9ff-6286fcfd7957"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weighted Hard Voting Score  1.000000\n",
            "Weighted Soft Voting Score  1.000000\n"
          ]
        }
      ],
      "source": [
        "# Weighted Voting Classifier with hard voting\n",
        "weights = [0.9, 0.8, 0.76]\n",
        "vot_hard = VotingClassifier(estimators = estimator, weights = weights, voting ='hard')\n",
        "vot_hard.fit(X_train, y_train)\n",
        "y_pred = vot_hard.predict(X_test)\n",
        "\n",
        "# using accuracy_score metric to predict accuracy\n",
        "score = accuracy_score(y_test, y_pred)\n",
        "print(\"Weighted Hard Voting Score % f\" % score)\n",
        "\n",
        "# Voting Classifier with soft voting\n",
        "weights = [0.9, 0.8, 0.76]\n",
        "vot_soft = VotingClassifier(estimators = estimator, weights = weights, voting ='soft')\n",
        "vot_soft.fit(X_train, y_train)\n",
        "y_pred = vot_soft.predict(X_test)\n",
        "\n",
        "# using accuracy_score\n",
        "score = accuracy_score(y_test, y_pred)\n",
        "print(\"Weighted Soft Voting Score % f\" % score)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7787440d",
      "metadata": {
        "id": "7787440d"
      },
      "source": [
        "### Advantages:\n",
        "- The voting architecture is straightforward to set up, especially when compared to stacking and blending, and it generally doesn't demand intricate fine-tuning.\n",
        "\n",
        "- Employing multiple base learners in the voting system reduces vulnerability to the impact of individual models, enhancing the stability and reliability of predictions.\n",
        "\n",
        "### Disadvantages:\n",
        "- Handling conflicts in predictions among the models can pose a challenge, making it tricky to arrive at a meaningful final decision.\n",
        "\n",
        "- Increasing the number of models in the ensemble voting model doesn't always lead to an improvement in the final performance, which can limit its effectiveness."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8b9e12b",
      "metadata": {
        "id": "a8b9e12b"
      },
      "source": [
        "## Bagging Method\n",
        "\n",
        "This approach is also referred to as the bootstrapping method. In this method, base models are applied to bags, which ensures a representative sampling of the entire dataset. A **`bag`** represents a subset of the dataset, and it includes replacements to match the size of the complete dataset. The final result is generated by consolidating the outputs of all the base models.\n",
        "\n",
        "### Algorithm:\n",
        "\n",
        "- Generate several datasets by randomly selecting observations from the training dataset with replacement.\n",
        "\n",
        "- Apply a base model to each of these generated datasets independently.\n",
        "\n",
        "- Aggregate the predictions from all the base models to produce the final output.\n",
        "\n",
        "Bagging normally uses only one base model (XGBoost Regressor used in the code below).\n",
        "\n",
        "<br>\n",
        "<img src=\"https://github.com/EDGE-Programe/Python-Basics/blob/master/Python_edge_program/Data%20Science/notebook_images/nb_24/ensemble_bagging.webp?raw=1\" alt=\"Bagging\" width=76% height=85% title=\"Bagging Method\">\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f01a805",
      "metadata": {
        "id": "1f01a805"
      },
      "outputs": [],
      "source": [
        "# training data\n",
        "X = dataset.iloc[:, 0:10].values\n",
        "y = dataset.iloc[:, [10,11,12,13,14]].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00c16426",
      "metadata": {
        "id": "00c16426",
        "outputId": "b7006f5a-5fe9-4f60-f0e3-22d635322505"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/rubayet/miniconda3/envs/edge/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.04805340355619929\n"
          ]
        }
      ],
      "source": [
        "# importing utility modules\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        "\n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "\n",
        "# Splitting between train data into training and validation dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        "\n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# predicting the output on the test dataset\n",
        "pred = model.predict(X_test)\n",
        "\n",
        "# printing the mean squared error between real value and predicted value\n",
        "print(mean_squared_error(y_test, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "205dd156",
      "metadata": {
        "id": "205dd156"
      },
      "source": [
        "**`Aggregation:`** This is a step that involves the process of combining the output of all base models and, based on their output, predicting an aggregate result with greater accuracy and reduced variance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "880b56fc",
      "metadata": {
        "id": "880b56fc"
      },
      "source": [
        "<br>\n",
        "<img src=\"https://github.com/EDGE-Programe/Python-Basics/blob/master/Python_edge_program/Data%20Science/notebook_images/nb_24/ensemble_bagging2.png?raw=1\" alt=\"Bagging\" width=67% height=58% title=\"Bagging Method\">\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "193385e2",
      "metadata": {
        "id": "193385e2"
      },
      "source": [
        "### Advantages:\n",
        "\n",
        "- The modeling process is simple and doesn't require complex mathematical concepts, plus it can handle missing values.\n",
        "\n",
        "- Implementation is made easy by the scikit-learn package, which includes modules for combining predictions from each base learner.\n",
        "\n",
        "- Bagging has a notable impact on reducing variance in high-variance classifiers, particularly beneficial for high-dimensional data, where it helps prevent overfitting to new data.\n",
        "\n",
        "- Bagging provides an unbiased estimate of the out-of-bag error, which represents the average error or loss across all these classifiers.\n",
        "\n",
        "### Disadvantages:\n",
        "\n",
        "- Bagging can be computationally intensive as it involves using multiple models.\n",
        "\n",
        "- The process of averaging predictions can make it challenging to interpret the final result, reducing the model's transparency."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33f1b501",
      "metadata": {
        "id": "33f1b501"
      },
      "source": [
        "## Stacking Method\n",
        "\n",
        "Stacking is an ensemble technique that merges various models, whether they are for classification or regression, using a meta-model, which could be a meta-classifier or a meta-regression model. To implement stacking, the base models are initially trained on the entire dataset, and subsequently, the meta-model is trained on the features generated as output by the base models. It's important to note that the base models employed in stacking are usually diverse in nature. The role of the meta-model is to discern and utilize the most informative features derived from the base models in order to attain the highest possible level of accuracy.\n",
        "<br>\n",
        "<img src=\"https://github.com/EDGE-Programe/Python-Basics/blob/master/Python_edge_program/Data%20Science/notebook_images/nb_24/Stacking_ensemble.png?raw=1\" alt=\"Bagging\" width=67% height=58% title=\"Bagging Method\">\n",
        "<br>\n",
        "\n",
        "**Stacking Algorithm:**\n",
        "- Divide the training dataset into **n** subsets.\n",
        "<br>\n",
        "- Train a base model, such as linear regression, on **n-1** of these subsets and make predictions for the remaining nth subset. Repeat this process for each of the n subsets in the training set.\n",
        "<br>\n",
        "- Fit the base model using the entire training dataset.\n",
        "<br>\n",
        "- Utilize this model to make predictions on the test dataset.\n",
        "<br>\n",
        "- Repeat steps 2 to 4 for another base model, resulting in another set of predictions for both the training and test datasets.\n",
        "<br>\n",
        "- Use the predictions made on the training dataset as additional features to construct a new model.\n",
        "<br>\n",
        "- Employ this final model to generate predictions on the test dataset.\n",
        "\n",
        "Stacking is a bit different from the basic ensembling methods because it has first-level and second-level models. Stacking features are first extracted by training the dataset with all the first-level models. A first-level model is then using the train stacking features to train the model than this model predicts the final output with test stacking features.\n",
        "<br>\n",
        "<img src=\"https://github.com/EDGE-Programe/Python-Basics/blob/master/Python_edge_program/Data%20Science/notebook_images/nb_24/stacking.png?raw=1\" alt=\"Bagging\" width=67% height=58% title=\"Bagging Method\">\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0433f8d",
      "metadata": {
        "id": "b0433f8d",
        "outputId": "43c585c1-b051-40af-cdcb-d8c5df67f649"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting vecstack\n",
            "  Downloading vecstack-0.4.0.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: numpy in /home/rubayet/miniconda3/envs/edge/lib/python3.8/site-packages (from vecstack) (1.23.5)\n",
            "Requirement already satisfied: scipy in /home/rubayet/miniconda3/envs/edge/lib/python3.8/site-packages (from vecstack) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /home/rubayet/miniconda3/envs/edge/lib/python3.8/site-packages (from vecstack) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /home/rubayet/miniconda3/envs/edge/lib/python3.8/site-packages (from scikit-learn>=0.18->vecstack) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/rubayet/miniconda3/envs/edge/lib/python3.8/site-packages (from scikit-learn>=0.18->vecstack) (3.1.0)\n",
            "Building wheels for collected packages: vecstack\n",
            "  Building wheel for vecstack (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for vecstack: filename=vecstack-0.4.0-py3-none-any.whl size=19864 sha256=7486a00f4e271c2316581745f42d36ac1f61413020c629f2782f6d34d03a10a1\n",
            "  Stored in directory: /home/rubayet/.cache/pip/wheels/17/89/0b/21d5484cbf713c95b641ec1bdc40dd7ae798cbdea2337e3535\n",
            "Successfully built vecstack\n",
            "Installing collected packages: vecstack\n",
            "Successfully installed vecstack-0.4.0\n"
          ]
        }
      ],
      "source": [
        "! pip install vecstack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "760cfd54",
      "metadata": {
        "id": "760cfd54",
        "outputId": "d6e6fe6e-5aa9-493e-9ac9-4beaa3ee1711"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/rubayet/miniconda3/envs/edge/lib/python3.8/site-packages/sklearn/datasets/_openml.py:303: UserWarning: Multiple active versions of the dataset matching the name boston exist. Versions may be fundamentally different, returning version 1.\n",
            "  warn(\n",
            "/home/rubayet/miniconda3/envs/edge/lib/python3.8/site-packages/sklearn/datasets/_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Boston Housing dataset\n",
        "boston = fetch_openml(name='boston')\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(boston.data, boston.target, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c3e5bd5",
      "metadata": {
        "id": "3c3e5bd5",
        "outputId": "f0c57668-ac50-45b2-bced-3a6fe3619665"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(random_state=42)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "DecisionTreeRegressor(random_state=42)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "\n",
        "# Train the base Decision Tree Model\n",
        "dt = DecisionTreeRegressor(random_state=42)\n",
        "dt.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba746951",
      "metadata": {
        "id": "ba746951",
        "outputId": "9eb77d85-8251-4710-c32a-17821cdd3ef4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(random_state=42)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "RandomForestRegressor(random_state=42)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the base RandomForestRegressor Model\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "rf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3f6152d",
      "metadata": {
        "id": "c3f6152d",
        "outputId": "92c1bc08-3ac8-48d7-89e8-854b26ff7a71"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(random_state=42)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "GradientBoostingRegressor(random_state=42)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the base GradientBoostingRegressor Model\n",
        "gb = GradientBoostingRegressor(random_state=42)\n",
        "gb.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f52bfaee",
      "metadata": {
        "id": "f52bfaee"
      },
      "outputs": [],
      "source": [
        "# Make predictions on the validation set\n",
        "dt_pred = dt.predict(X_val)\n",
        "rf_pred = rf.predict(X_val)\n",
        "gb_pred = gb.predict(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97c278be",
      "metadata": {
        "id": "97c278be",
        "outputId": "1fa79c0b-b365-4521-81b4-9bc9caaeebe1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-7 {color: black;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the meta model\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Combine the predictions of the base models into a single feature matrix\n",
        "X_val_meta = np.column_stack((dt_pred, rf_pred, gb_pred))\n",
        "\n",
        "# Train the meta-model on the combined feature matrix and the target values\n",
        "meta_model = LinearRegression()\n",
        "meta_model.fit(X_val_meta, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ca578bd",
      "metadata": {
        "id": "4ca578bd",
        "outputId": "827dd19e-aa65-4f4a-cfb6-3476bc58544f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 13)\n",
            "Predicted median value of owner-occupied homes: $49.75 thousand\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/rubayet/miniconda3/envs/edge/lib/python3.8/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but DecisionTreeRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/home/rubayet/miniconda3/envs/edge/lib/python3.8/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/home/rubayet/miniconda3/envs/edge/lib/python3.8/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Make predictions on new data\n",
        "X_new = np.array([[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3]])\n",
        "print(X_new.shape)\n",
        "dt_pred_new = dt.predict(X_new)\n",
        "rf_pred_new = rf.predict(X_new)\n",
        "gb_pred_new = gb.predict(X_new)\n",
        "\n",
        "# Combine the predictions of the base models into a single feature matrix\n",
        "X_new_meta = np.column_stack((dt_pred_new, rf_pred_new, gb_pred_new))\n",
        "\n",
        "# Make a prediction using the meta-model\n",
        "y_new_pred = meta_model.predict(X_new_meta)\n",
        "\n",
        "print(\"Predicted median value of owner-occupied homes: ${:.2f} thousand\".format(y_new_pred[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cbd2b11",
      "metadata": {
        "id": "8cbd2b11"
      },
      "source": [
        "# An Overview of Popular Ensemble Algorithms\n",
        "In the preceding sections, we discussed various types of ensemble models. Now, let's provide a concise overview of some popular models.\n",
        "\n",
        "## Random Forest\n",
        "Random Forest is a widely used model capable of tackling both classification and regression problems. It consists of multiple decision trees trained using a technique called bagging. The final prediction of a Random Forest is obtained by averaging the predictions made by individual trees.\n",
        "\n",
        "Before training a Random Forest, there are three key hyperparameters to set: **node size**, **the number of trees**, and **the number of features to be sampled**.\n",
        "\n",
        "Randomness is introduced through a process known as feature bagging or feature randomness. This method selects a random subset of features to ensure low correlation among the decision trees. It sets Random Forests apart from decision trees, which consider all possible feature splits, whereas Random Forests only consider a subset of these features.\n",
        "\n",
        "## XGBoost\n",
        "Extreme Gradient Boosting, abbreviated as XGBoost, serves both classification and regression tasks. XGBoost is designed to be highly efficient and scalable, implementing the gradient boosting decision tree framework.\n",
        "\n",
        "It is particularly suitable for handling large-scale datasets and is compatible with major distributed environments like Hadoop, MPI (Message Passing Inference), and SGI (Sun Grid Engine).\n",
        "\n",
        "## AdaBoost\n",
        "AdaBoost, short for Adaptive Boosting, stands as one of the earliest ensemble boosting classifiers designed for successful binary classification. Its **'adaptive'** nature lies in the fact that it reassigns weights to each instance, giving higher weights to misclassified instances."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "edge",
      "language": "python",
      "name": "edge"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}